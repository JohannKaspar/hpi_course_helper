{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from Module import CourseModule\n",
    "importlib.reload(sys.modules['Module'])\n",
    "from scrape_helper import get_module_links, get_courses\n",
    "from helpers import courses_dict\n",
    "import sqlite3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Div with class 'tx-ciuniversity' not found for url: https://hpi.de//studium/im-studium/lehrveranstaltungen/professional-skills.html\n",
      "No modules found for course: https://hpi.de//studium/im-studium/lehrveranstaltungen/professional-skills.html\n",
      "Div with class 'tx-ciuniversity' not found for url: https://hpi.de/entrepreneurship/home.html\n",
      "No modules found for course: https://hpi.de/entrepreneurship/home.html\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all courses from the given url\n",
    "courses = get_courses(\"https://hpi.de/studium/im-studium/lehrveranstaltungen.html\")\n",
    "\n",
    "# Dictionary to store the courses and their modules (trimmed links)\n",
    "course_modules = {}\n",
    "\n",
    "# Dictionary to store trimmed and full module links\n",
    "module_links_dict = {}\n",
    "\n",
    "for course in courses:\n",
    "\n",
    "    # get the list of modules for the given course\n",
    "    module_links = get_module_links(course)\n",
    "\n",
    "    # If no modules were found, skip the course\n",
    "    if module_links == []:\n",
    "        print(\"No modules found for course:\", course)\n",
    "        continue\n",
    "\n",
    "    # Cut the host part of the url\n",
    "    module_links_trimmed = [link.split(\"/\")[-1] for link in module_links]\n",
    "\n",
    "    # add the trimmed and full links to the dictionary\n",
    "    for trimmed, full in zip(module_links_trimmed, module_links):\n",
    "        module_links_dict[trimmed] = full\n",
    "    \n",
    "    # add the course and its modules to the dictionary\n",
    "    course_modules[course] = module_links_trimmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for module 1/115\n",
      "Fetching data for module 11/115\n",
      "Fetching data for module 21/115\n",
      "Fetching data for module 31/115\n",
      "Fetching data for module 41/115\n",
      "Fetching data for module 51/115\n",
      "Fetching data for module 61/115\n",
      "Fetching data for module 71/115\n",
      "Fetching data for module 81/115\n",
      "Fetching data for module 91/115\n",
      "Fetching data for module 101/115\n",
      "Fetching data for module 111/115\n"
     ]
    }
   ],
   "source": [
    "# create a list of modules of class Module\n",
    "modules = {}\n",
    "\n",
    "# create a Module object for each module\n",
    "for i, (url_trimmed, url) in enumerate(module_links_dict.items()):\n",
    "    if i%10 == 0:\n",
    "        print(f\"Fetching data for module {i+1}/{len(module_links_dict)}\")\n",
    "\n",
    "    # create a Module object for the given url\n",
    "    module = CourseModule(url)\n",
    "    module.get_landing_page_information()\n",
    "    module.get_evaluation_metrics()\n",
    "\n",
    "    # add the module to the list of modules\n",
    "    modules[url_trimmed] = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the database if it already exists\n",
    "try:\n",
    "    os.remove(\"hpi_modules.db\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "connection = sqlite3.connect(\"hpi_modules.db\")\n",
    "with open('sqlite_db_setup.sql', 'r') as sql_file:\n",
    "    sql_queries = sql_file.read()\n",
    "connection.executescript(sql_queries)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('itse_bachelor', 'IT-Systems Engineering BA')\n",
      "('itse_master', 'IT-Systems Engineering MA')\n",
      "('dh_master', 'Digital Health MA')\n",
      "('de_master', 'Data Engineering MA')\n",
      "('cyber_master', 'Cybersecurity MA')\n",
      "('sse_master', 'Software Systems Engineering MA')\n"
     ]
    }
   ],
   "source": [
    "cursor = connection.cursor()\n",
    "try:\n",
    "    cursor.executemany(\"INSERT INTO courses VALUES (?, ?)\", zip(courses_dict.values(), courses_dict.keys()))\n",
    "    connection.commit()\n",
    "except sqlite3.IntegrityError:\n",
    "    print(\"Courses already exist in the database\")\n",
    "for row in cursor.execute(\"SELECT * FROM courses\"):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in modules.values():\n",
    "    try:\n",
    "        cursor.execute(\"INSERT INTO modules VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "                    (module.url_trimmed,\n",
    "                    module.url,\n",
    "                    module.website,\n",
    "                    module.title,\n",
    "                    module.credits,\n",
    "                    module.evap_grade,\n",
    "                    module.evap_semester,\n",
    "                    module.description,\n",
    "                    module.lecturers))\n",
    "        connection.commit()\n",
    "    except sqlite3.IntegrityError:\n",
    "        print(\"Module already exists in database: \", module.url_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntegrityError: itse_bachelor, wise-23-24-3846-3d_computergrafik-i.html, HCGT, \n"
     ]
    }
   ],
   "source": [
    "for module in modules.values():\n",
    "    for module_group in module.module_groups:\n",
    "        try:\n",
    "            cursor.execute(\"INSERT INTO course_modules VALUES (?, ?, ?, ?)\",\n",
    "                        [\n",
    "                            module_group[0],    # course abbreviation\n",
    "                            module.url_trimmed, # module url trimmed\n",
    "                            module_group[1],    # module group\n",
    "                            module_group[2]     # submodule group\n",
    "                        ]) \n",
    "            connection.commit()\n",
    "        except sqlite3.IntegrityError:\n",
    "            print(f\"IntegrityError: {module_group[0]}, {module.url_trimmed}, {module_group[1]}, {module_group[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', None, 'Feature Selection and Classification for hierarchically structured feature spaces', 6, 1.1, 'WiSe 2025/26', '------------------------------------------------------------------------------------------------------------------------------------\\nIf you want to join this seminar, please enroll to the moodle course:\\nhttps://moodle.hpi.de/course/view.php?id=688\\n------------------------------------------------------------------------------------------------------------------------------------\\nPowerful predictive models based on high-dimensional data are a cornerstone of modern machine learning applications. Yet, in real-world scenarios, often a vast majority of the available features are irrelevant or redundant to others, rendering them superfluous for the actual prediction task. While some model classes (e.g., decision trees) intrinsically select the most relevant features, others (e.g., non-penalized regression models or neural networks) have difficulties in identifying these, thus resulting in less desirable model properties such as overfitting, prediction uncertainty, or decreased feature set stability.\\nOne way to successfully remedy this shortcoming is feature selection. Feature selection aims at reducing a large number of input features to a small set of relevant and non-redundant ones, e.g., by selecting according to data-intrinsic properties (filter methods) by fitting separate models to subsets of the feature space (wrapper methods), or by designing models that perform feature selection intrinsically (embedded methods).\\nHowever, there are scenarios where the features themselves additionally show hierarchical relations amongst each other, i.e., one feature is a more specific instance of a more general feature. Two frequently mentioned examples in this context are:\\nIn these settings, selecting both the more specific and the more general concept for modeling would lead to (hierarchical) redundancy, and should thus be avoided through the feature selection process. Unfavorably, all of the well-known ‘flat’ feature selection methods have limited capabilities in realizing this, opening up the field for specialized hierarchical features selection (HFS) methods. They take as input the original features as well as the information about the feature hierarchy, organized in a tree or a directed acyclic graph (DAG) – in the two above-mentioned examples the GeneOntology and WordNet.\\nAround two dozen papers about these HFS methods have been published over the last two decades, but openly accessible implementations are only available for a few. As a consequence, the available filter and wrapper HFS methods are currently being implemented in Python as a scikit-learn compatible open source library here at HPI during summer term 2023.\\nThe first goal of this master seminar will thus be to extend this package and include embedded HFS methods as well as specialized classifiers that are designed to work out-of-the-box with hierarchical feature spaces (without implicit feature selection). The second goal of the project is to improve the existing methods in this area of hierarchically structured input data on both conceptual and computational levels.\\nFor that purpose, the project will be structured as follows: You will first learn the fundamentals of HFS. In a second step, you will familiarize yourself with the design principles of Python libraries in general, and the scikit-learn environment and the currently developed HFS library in particular. In the third place, each participant of the seminar will implement a subset of these methods compatible with the existing library, and perform thorough testing and benchmarking, including the application on real-world datasets. Fourthly, you will develop ideas about the improvement of the existing methods (or the design of new ones), implement these and design a set of experiments to collect evidence convincingly showing the hypothesized improvements. Lastly, your implementation will be added to the technical documentation of the library, and your experiments and results will be written up in a self-contained scientific project report.\\nBy the end of the semester, you will have completed the following deliverables:\\n\\nOptional:\\n\\nLearning Objectives:\\nUnderstand the rationale for choosing different feature selection techniques for hierarchically structured data and their differences to ‘flat’ feature selection methods.- Ability to develop meaningful benchmarks for testing the practically relevant aspects of different methods.- Ability to extract and distill important information from scientific literature.- Ability to develop a research question based on existing evidence, to set up and run an experiment to address this question, and to critically assess the results.- Gain experience in collaboratively working on a science-oriented software project geared towards a broader spectrum of users.', 'Prof. Dr. Bernhard Renard, Jan-Philipp Sachs', 'dh_master', 'wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'SCAD', 'C', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', None, 'Feature Selection and Classification for hierarchically structured feature spaces', 6, 1.1, 'WiSe 2025/26', '------------------------------------------------------------------------------------------------------------------------------------\\nIf you want to join this seminar, please enroll to the moodle course:\\nhttps://moodle.hpi.de/course/view.php?id=688\\n------------------------------------------------------------------------------------------------------------------------------------\\nPowerful predictive models based on high-dimensional data are a cornerstone of modern machine learning applications. Yet, in real-world scenarios, often a vast majority of the available features are irrelevant or redundant to others, rendering them superfluous for the actual prediction task. While some model classes (e.g., decision trees) intrinsically select the most relevant features, others (e.g., non-penalized regression models or neural networks) have difficulties in identifying these, thus resulting in less desirable model properties such as overfitting, prediction uncertainty, or decreased feature set stability.\\nOne way to successfully remedy this shortcoming is feature selection. Feature selection aims at reducing a large number of input features to a small set of relevant and non-redundant ones, e.g., by selecting according to data-intrinsic properties (filter methods) by fitting separate models to subsets of the feature space (wrapper methods), or by designing models that perform feature selection intrinsically (embedded methods).\\nHowever, there are scenarios where the features themselves additionally show hierarchical relations amongst each other, i.e., one feature is a more specific instance of a more general feature. Two frequently mentioned examples in this context are:\\nIn these settings, selecting both the more specific and the more general concept for modeling would lead to (hierarchical) redundancy, and should thus be avoided through the feature selection process. Unfavorably, all of the well-known ‘flat’ feature selection methods have limited capabilities in realizing this, opening up the field for specialized hierarchical features selection (HFS) methods. They take as input the original features as well as the information about the feature hierarchy, organized in a tree or a directed acyclic graph (DAG) – in the two above-mentioned examples the GeneOntology and WordNet.\\nAround two dozen papers about these HFS methods have been published over the last two decades, but openly accessible implementations are only available for a few. As a consequence, the available filter and wrapper HFS methods are currently being implemented in Python as a scikit-learn compatible open source library here at HPI during summer term 2023.\\nThe first goal of this master seminar will thus be to extend this package and include embedded HFS methods as well as specialized classifiers that are designed to work out-of-the-box with hierarchical feature spaces (without implicit feature selection). The second goal of the project is to improve the existing methods in this area of hierarchically structured input data on both conceptual and computational levels.\\nFor that purpose, the project will be structured as follows: You will first learn the fundamentals of HFS. In a second step, you will familiarize yourself with the design principles of Python libraries in general, and the scikit-learn environment and the currently developed HFS library in particular. In the third place, each participant of the seminar will implement a subset of these methods compatible with the existing library, and perform thorough testing and benchmarking, including the application on real-world datasets. Fourthly, you will develop ideas about the improvement of the existing methods (or the design of new ones), implement these and design a set of experiments to collect evidence convincingly showing the hypothesized improvements. Lastly, your implementation will be added to the technical documentation of the library, and your experiments and results will be written up in a self-contained scientific project report.\\nBy the end of the semester, you will have completed the following deliverables:\\n\\nOptional:\\n\\nLearning Objectives:\\nUnderstand the rationale for choosing different feature selection techniques for hierarchically structured data and their differences to ‘flat’ feature selection methods.- Ability to develop meaningful benchmarks for testing the practically relevant aspects of different methods.- Ability to extract and distill important information from scientific literature.- Ability to develop a research question based on existing evidence, to set up and run an experiment to address this question, and to critically assess the results.- Gain experience in collaboratively working on a science-oriented software project geared towards a broader spectrum of users.', 'Prof. Dr. Bernhard Renard, Jan-Philipp Sachs', 'dh_master', 'wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'SCAD', 'T', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', None, 'Feature Selection and Classification for hierarchically structured feature spaces', 6, 1.1, 'WiSe 2025/26', '------------------------------------------------------------------------------------------------------------------------------------\\nIf you want to join this seminar, please enroll to the moodle course:\\nhttps://moodle.hpi.de/course/view.php?id=688\\n------------------------------------------------------------------------------------------------------------------------------------\\nPowerful predictive models based on high-dimensional data are a cornerstone of modern machine learning applications. Yet, in real-world scenarios, often a vast majority of the available features are irrelevant or redundant to others, rendering them superfluous for the actual prediction task. While some model classes (e.g., decision trees) intrinsically select the most relevant features, others (e.g., non-penalized regression models or neural networks) have difficulties in identifying these, thus resulting in less desirable model properties such as overfitting, prediction uncertainty, or decreased feature set stability.\\nOne way to successfully remedy this shortcoming is feature selection. Feature selection aims at reducing a large number of input features to a small set of relevant and non-redundant ones, e.g., by selecting according to data-intrinsic properties (filter methods) by fitting separate models to subsets of the feature space (wrapper methods), or by designing models that perform feature selection intrinsically (embedded methods).\\nHowever, there are scenarios where the features themselves additionally show hierarchical relations amongst each other, i.e., one feature is a more specific instance of a more general feature. Two frequently mentioned examples in this context are:\\nIn these settings, selecting both the more specific and the more general concept for modeling would lead to (hierarchical) redundancy, and should thus be avoided through the feature selection process. Unfavorably, all of the well-known ‘flat’ feature selection methods have limited capabilities in realizing this, opening up the field for specialized hierarchical features selection (HFS) methods. They take as input the original features as well as the information about the feature hierarchy, organized in a tree or a directed acyclic graph (DAG) – in the two above-mentioned examples the GeneOntology and WordNet.\\nAround two dozen papers about these HFS methods have been published over the last two decades, but openly accessible implementations are only available for a few. As a consequence, the available filter and wrapper HFS methods are currently being implemented in Python as a scikit-learn compatible open source library here at HPI during summer term 2023.\\nThe first goal of this master seminar will thus be to extend this package and include embedded HFS methods as well as specialized classifiers that are designed to work out-of-the-box with hierarchical feature spaces (without implicit feature selection). The second goal of the project is to improve the existing methods in this area of hierarchically structured input data on both conceptual and computational levels.\\nFor that purpose, the project will be structured as follows: You will first learn the fundamentals of HFS. In a second step, you will familiarize yourself with the design principles of Python libraries in general, and the scikit-learn environment and the currently developed HFS library in particular. In the third place, each participant of the seminar will implement a subset of these methods compatible with the existing library, and perform thorough testing and benchmarking, including the application on real-world datasets. Fourthly, you will develop ideas about the improvement of the existing methods (or the design of new ones), implement these and design a set of experiments to collect evidence convincingly showing the hypothesized improvements. Lastly, your implementation will be added to the technical documentation of the library, and your experiments and results will be written up in a self-contained scientific project report.\\nBy the end of the semester, you will have completed the following deliverables:\\n\\nOptional:\\n\\nLearning Objectives:\\nUnderstand the rationale for choosing different feature selection techniques for hierarchically structured data and their differences to ‘flat’ feature selection methods.- Ability to develop meaningful benchmarks for testing the practically relevant aspects of different methods.- Ability to extract and distill important information from scientific literature.- Ability to develop a research question based on existing evidence, to set up and run an experiment to address this question, and to critically assess the results.- Gain experience in collaboratively working on a science-oriented software project geared towards a broader spectrum of users.', 'Prof. Dr. Bernhard Renard, Jan-Philipp Sachs', 'dh_master', 'wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'SCAD', 'S', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', None, 'Feature Selection and Classification for hierarchically structured feature spaces', 6, 1.1, 'WiSe 2025/26', '------------------------------------------------------------------------------------------------------------------------------------\\nIf you want to join this seminar, please enroll to the moodle course:\\nhttps://moodle.hpi.de/course/view.php?id=688\\n------------------------------------------------------------------------------------------------------------------------------------\\nPowerful predictive models based on high-dimensional data are a cornerstone of modern machine learning applications. Yet, in real-world scenarios, often a vast majority of the available features are irrelevant or redundant to others, rendering them superfluous for the actual prediction task. While some model classes (e.g., decision trees) intrinsically select the most relevant features, others (e.g., non-penalized regression models or neural networks) have difficulties in identifying these, thus resulting in less desirable model properties such as overfitting, prediction uncertainty, or decreased feature set stability.\\nOne way to successfully remedy this shortcoming is feature selection. Feature selection aims at reducing a large number of input features to a small set of relevant and non-redundant ones, e.g., by selecting according to data-intrinsic properties (filter methods) by fitting separate models to subsets of the feature space (wrapper methods), or by designing models that perform feature selection intrinsically (embedded methods).\\nHowever, there are scenarios where the features themselves additionally show hierarchical relations amongst each other, i.e., one feature is a more specific instance of a more general feature. Two frequently mentioned examples in this context are:\\nIn these settings, selecting both the more specific and the more general concept for modeling would lead to (hierarchical) redundancy, and should thus be avoided through the feature selection process. Unfavorably, all of the well-known ‘flat’ feature selection methods have limited capabilities in realizing this, opening up the field for specialized hierarchical features selection (HFS) methods. They take as input the original features as well as the information about the feature hierarchy, organized in a tree or a directed acyclic graph (DAG) – in the two above-mentioned examples the GeneOntology and WordNet.\\nAround two dozen papers about these HFS methods have been published over the last two decades, but openly accessible implementations are only available for a few. As a consequence, the available filter and wrapper HFS methods are currently being implemented in Python as a scikit-learn compatible open source library here at HPI during summer term 2023.\\nThe first goal of this master seminar will thus be to extend this package and include embedded HFS methods as well as specialized classifiers that are designed to work out-of-the-box with hierarchical feature spaces (without implicit feature selection). The second goal of the project is to improve the existing methods in this area of hierarchically structured input data on both conceptual and computational levels.\\nFor that purpose, the project will be structured as follows: You will first learn the fundamentals of HFS. In a second step, you will familiarize yourself with the design principles of Python libraries in general, and the scikit-learn environment and the currently developed HFS library in particular. In the third place, each participant of the seminar will implement a subset of these methods compatible with the existing library, and perform thorough testing and benchmarking, including the application on real-world datasets. Fourthly, you will develop ideas about the improvement of the existing methods (or the design of new ones), implement these and design a set of experiments to collect evidence convincingly showing the hypothesized improvements. Lastly, your implementation will be added to the technical documentation of the library, and your experiments and results will be written up in a self-contained scientific project report.\\nBy the end of the semester, you will have completed the following deliverables:\\n\\nOptional:\\n\\nLearning Objectives:\\nUnderstand the rationale for choosing different feature selection techniques for hierarchically structured data and their differences to ‘flat’ feature selection methods.- Ability to develop meaningful benchmarks for testing the practically relevant aspects of different methods.- Ability to extract and distill important information from scientific literature.- Ability to develop a research question based on existing evidence, to set up and run an experiment to address this question, and to critically assess the results.- Gain experience in collaboratively working on a science-oriented software project geared towards a broader spectrum of users.', 'Prof. Dr. Bernhard Renard, Jan-Philipp Sachs', 'dh_master', 'wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'APAD', 'C', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', None, 'Feature Selection and Classification for hierarchically structured feature spaces', 6, 1.1, 'WiSe 2025/26', '------------------------------------------------------------------------------------------------------------------------------------\\nIf you want to join this seminar, please enroll to the moodle course:\\nhttps://moodle.hpi.de/course/view.php?id=688\\n------------------------------------------------------------------------------------------------------------------------------------\\nPowerful predictive models based on high-dimensional data are a cornerstone of modern machine learning applications. Yet, in real-world scenarios, often a vast majority of the available features are irrelevant or redundant to others, rendering them superfluous for the actual prediction task. While some model classes (e.g., decision trees) intrinsically select the most relevant features, others (e.g., non-penalized regression models or neural networks) have difficulties in identifying these, thus resulting in less desirable model properties such as overfitting, prediction uncertainty, or decreased feature set stability.\\nOne way to successfully remedy this shortcoming is feature selection. Feature selection aims at reducing a large number of input features to a small set of relevant and non-redundant ones, e.g., by selecting according to data-intrinsic properties (filter methods) by fitting separate models to subsets of the feature space (wrapper methods), or by designing models that perform feature selection intrinsically (embedded methods).\\nHowever, there are scenarios where the features themselves additionally show hierarchical relations amongst each other, i.e., one feature is a more specific instance of a more general feature. Two frequently mentioned examples in this context are:\\nIn these settings, selecting both the more specific and the more general concept for modeling would lead to (hierarchical) redundancy, and should thus be avoided through the feature selection process. Unfavorably, all of the well-known ‘flat’ feature selection methods have limited capabilities in realizing this, opening up the field for specialized hierarchical features selection (HFS) methods. They take as input the original features as well as the information about the feature hierarchy, organized in a tree or a directed acyclic graph (DAG) – in the two above-mentioned examples the GeneOntology and WordNet.\\nAround two dozen papers about these HFS methods have been published over the last two decades, but openly accessible implementations are only available for a few. As a consequence, the available filter and wrapper HFS methods are currently being implemented in Python as a scikit-learn compatible open source library here at HPI during summer term 2023.\\nThe first goal of this master seminar will thus be to extend this package and include embedded HFS methods as well as specialized classifiers that are designed to work out-of-the-box with hierarchical feature spaces (without implicit feature selection). The second goal of the project is to improve the existing methods in this area of hierarchically structured input data on both conceptual and computational levels.\\nFor that purpose, the project will be structured as follows: You will first learn the fundamentals of HFS. In a second step, you will familiarize yourself with the design principles of Python libraries in general, and the scikit-learn environment and the currently developed HFS library in particular. In the third place, each participant of the seminar will implement a subset of these methods compatible with the existing library, and perform thorough testing and benchmarking, including the application on real-world datasets. Fourthly, you will develop ideas about the improvement of the existing methods (or the design of new ones), implement these and design a set of experiments to collect evidence convincingly showing the hypothesized improvements. Lastly, your implementation will be added to the technical documentation of the library, and your experiments and results will be written up in a self-contained scientific project report.\\nBy the end of the semester, you will have completed the following deliverables:\\n\\nOptional:\\n\\nLearning Objectives:\\nUnderstand the rationale for choosing different feature selection techniques for hierarchically structured data and their differences to ‘flat’ feature selection methods.- Ability to develop meaningful benchmarks for testing the practically relevant aspects of different methods.- Ability to extract and distill important information from scientific literature.- Ability to develop a research question based on existing evidence, to set up and run an experiment to address this question, and to critically assess the results.- Gain experience in collaboratively working on a science-oriented software project geared towards a broader spectrum of users.', 'Prof. Dr. Bernhard Renard, Jan-Philipp Sachs', 'dh_master', 'wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'APAD', 'T', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', None, 'Feature Selection and Classification for hierarchically structured feature spaces', 6, 1.1, 'WiSe 2025/26', '------------------------------------------------------------------------------------------------------------------------------------\\nIf you want to join this seminar, please enroll to the moodle course:\\nhttps://moodle.hpi.de/course/view.php?id=688\\n------------------------------------------------------------------------------------------------------------------------------------\\nPowerful predictive models based on high-dimensional data are a cornerstone of modern machine learning applications. Yet, in real-world scenarios, often a vast majority of the available features are irrelevant or redundant to others, rendering them superfluous for the actual prediction task. While some model classes (e.g., decision trees) intrinsically select the most relevant features, others (e.g., non-penalized regression models or neural networks) have difficulties in identifying these, thus resulting in less desirable model properties such as overfitting, prediction uncertainty, or decreased feature set stability.\\nOne way to successfully remedy this shortcoming is feature selection. Feature selection aims at reducing a large number of input features to a small set of relevant and non-redundant ones, e.g., by selecting according to data-intrinsic properties (filter methods) by fitting separate models to subsets of the feature space (wrapper methods), or by designing models that perform feature selection intrinsically (embedded methods).\\nHowever, there are scenarios where the features themselves additionally show hierarchical relations amongst each other, i.e., one feature is a more specific instance of a more general feature. Two frequently mentioned examples in this context are:\\nIn these settings, selecting both the more specific and the more general concept for modeling would lead to (hierarchical) redundancy, and should thus be avoided through the feature selection process. Unfavorably, all of the well-known ‘flat’ feature selection methods have limited capabilities in realizing this, opening up the field for specialized hierarchical features selection (HFS) methods. They take as input the original features as well as the information about the feature hierarchy, organized in a tree or a directed acyclic graph (DAG) – in the two above-mentioned examples the GeneOntology and WordNet.\\nAround two dozen papers about these HFS methods have been published over the last two decades, but openly accessible implementations are only available for a few. As a consequence, the available filter and wrapper HFS methods are currently being implemented in Python as a scikit-learn compatible open source library here at HPI during summer term 2023.\\nThe first goal of this master seminar will thus be to extend this package and include embedded HFS methods as well as specialized classifiers that are designed to work out-of-the-box with hierarchical feature spaces (without implicit feature selection). The second goal of the project is to improve the existing methods in this area of hierarchically structured input data on both conceptual and computational levels.\\nFor that purpose, the project will be structured as follows: You will first learn the fundamentals of HFS. In a second step, you will familiarize yourself with the design principles of Python libraries in general, and the scikit-learn environment and the currently developed HFS library in particular. In the third place, each participant of the seminar will implement a subset of these methods compatible with the existing library, and perform thorough testing and benchmarking, including the application on real-world datasets. Fourthly, you will develop ideas about the improvement of the existing methods (or the design of new ones), implement these and design a set of experiments to collect evidence convincingly showing the hypothesized improvements. Lastly, your implementation will be added to the technical documentation of the library, and your experiments and results will be written up in a self-contained scientific project report.\\nBy the end of the semester, you will have completed the following deliverables:\\n\\nOptional:\\n\\nLearning Objectives:\\nUnderstand the rationale for choosing different feature selection techniques for hierarchically structured data and their differences to ‘flat’ feature selection methods.- Ability to develop meaningful benchmarks for testing the practically relevant aspects of different methods.- Ability to extract and distill important information from scientific literature.- Ability to develop a research question based on existing evidence, to set up and run an experiment to address this question, and to critically assess the results.- Gain experience in collaboratively working on a science-oriented software project geared towards a broader spectrum of users.', 'Prof. Dr. Bernhard Renard, Jan-Philipp Sachs', 'dh_master', 'wise-23-24-3858-feature-selection-and-classification-for-hierarchically-structured-feature-spaces.html', 'APAD', 'S', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3940-founder-fundamentals.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3940-founder-fundamentals.html', 'http://hpi.de/entrepreneurship/founder-fundamentals', 'Founder Fundamentals', 3, 1.1, 'WS 22/23', None, 'Dr. Frank Pawlitschek, (School of Entrepreneurship), http://hpi.de/entrepreneurship/founder-fundamentals', 'dh_master', 'wise-23-24-3940-founder-fundamentals.html', 'PSK', 'ML', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3934-intrapersonelle--interpersonelle-kompetenzen.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3934-intrapersonelle--interpersonelle-kompetenzen.html', None, 'Intrapersonelle & Interpersonelle Kompetenzen', 3, 1.1, 'WS 22/23', '\\xadJeder Mensch hat unterschiedliche Vorgehensweisen bei der Bearbeitung von Aufgaben und beim Umgang mit herausfordernden Situationen. Einerseits werden diese Muster durch unsere Erfahrungen geprägt. Sie legen nahe, wann und wie wir Probleme lösen, welche Schwerpunkte wir dabei setzen und welchen “Preis” wir dafür zahlen. Andererseits prägen wir mit diesen Vorgehensweisen unsere Beziehungen, die Kommunikation und die Wirkungen gegenüber Anderen. Wir widmen uns den sogenannten „Metakompetenzen“ und der (zwischen)menschlichen „Software“ mit folgenden Schwerpunkten:\\nA) Menschen mit ausgeprägten intrapersonellen Kompetenzen sind sich ihrer Stärken, Motivationen, Emotionen und Bedürfnisse sowie deren Ursachen bewusst. Sie reflektieren sich regelmäßig, nutzen ihre Intuition und haben Freude daran, selbstbewusst, ihre Potenziale weiter zu entfalten. Lernziele, Sie:\\nentwickeln eine Grundausrichtung, die Kraft gibt, zieht und inspiriert.- erarbeiten sich mentale Stärke und Strategien für Resilienz.- fokussieren Ihre Aufmerksamkeit auf das Wesentliche.- erkunden Ihren optimalen Leistungszustand\\nB) Menschen mit hohen interpersonellen Kompetenzen gestalten Interaktionen erfolgreich. Sie verstehen zwischenmenschliche Beziehungen, können gut Perspektiven wechseln, kommunizieren zielführend, lösen Spannungen auf und gestalten Gruppensituationen mit Positivität und Kreativität. Lernziele, Sie:\\nverbessern Ihre Kommunikation und erhöhen Ihre Durchsetzungskraft.- trainieren Ihre Empathie, kreative Perspektivenwechsel und verschiedene Dimensionen emotionaler Intelligenz.- erlernen das Überbringen unangenehmer Botschaften und Strategien zum Auflösen konfliktbehafteter Situationen.- trainieren Fähigkeiten für den Umgang mit Fehlern, der Potenzialentfaltung bei Veränderungsvorhaben und der Teamenergie.- stabilisieren ein entwicklungsorientiertes, agiles und kollaboratives Mind-Set.\\nDozentin\\nDr. Jana Leidenfrost:\\nTrainerin und Beraterin im Feld der Internationalen Führungskräfteentwicklung sowie Psychologisches Mentoring in Hochleistungsbereichen. Lehrbeauftragte an verschiedenen Hochschulen; zwischen 2000 und 2010 Managerin, Trainerin und Coach im Feld der Führungskräfteentwicklung der Daimler AG', 'Dr. Jana Leidenfrost', 'dh_master', 'wise-23-24-3934-intrapersonelle--interpersonelle-kompetenzen.html', 'PSK', 'CO', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3845-mobilkommunikation.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3845-mobilkommunikation.html', None, 'Mobilkommunikation', 6, 1.1, 'WS 21/22', None, 'Prof. Holger Karl', 'dh_master', 'wise-23-24-3845-mobilkommunikation.html', 'APAD', 'C', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3845-mobilkommunikation.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3845-mobilkommunikation.html', None, 'Mobilkommunikation', 6, 1.1, 'WS 21/22', None, 'Prof. Holger Karl', 'dh_master', 'wise-23-24-3845-mobilkommunikation.html', 'APAD', 'T', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3845-mobilkommunikation.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3845-mobilkommunikation.html', None, 'Mobilkommunikation', 6, 1.1, 'WS 21/22', None, 'Prof. Holger Karl', 'dh_master', 'wise-23-24-3845-mobilkommunikation.html', 'APAD', 'S', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3953-power-and-power-misuse-in-organizations.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3953-power-and-power-misuse-in-organizations.html', None, 'Power and Power Misuse in Organizations', 3, 1.1, 'WiSe 2025/26', '\\xad', 'Karsten Drath', 'dh_master', 'wise-23-24-3953-power-and-power-misuse-in-organizations.html', 'PSK', 'CO', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3953-power-and-power-misuse-in-organizations.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3953-power-and-power-misuse-in-organizations.html', None, 'Power and Power Misuse in Organizations', 3, 1.1, 'WiSe 2025/26', '\\xad', 'Karsten Drath', 'dh_master', 'wise-23-24-3953-power-and-power-misuse-in-organizations.html', 'PSK', 'ML', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3892-reverse-engineering-for-security-analysis.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3892-reverse-engineering-for-security-analysis.html', None, 'Reverse Engineering for Security Analysis', 6, 1.1, 'WiSe 2025/26', 'This seminar consists of two parts:\\nThe security research project can be done individually or in small groups (up to four students). Each group chooses a real-world target. Targets must be legal to reverse-engineer in the context of security research. Many companies offer bug bounty programs that explicitly allow reverse engineering, such as Apple and Microsoft. In case vulnerabilities are found, students coordinately disclose them to the vendors, thereby improving the security of popular software. Instead of reverse engineering a particular target, students can also choose to develop reverse engineering tools.\\nStudents are free to choose their security research method. If applicable, they can use fuzzing and even collaborate with students from the course\\xa0Open-Source Fuzzing. While fuzzing is generally great for finding memory safety issues, there are further bug classes that require other approaches.', 'Dr. Jiska Classen', 'dh_master', 'wise-23-24-3892-reverse-engineering-for-security-analysis.html', 'HDAS', 'T', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3892-reverse-engineering-for-security-analysis.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3892-reverse-engineering-for-security-analysis.html', None, 'Reverse Engineering for Security Analysis', 6, 1.1, 'WiSe 2025/26', 'This seminar consists of two parts:\\nThe security research project can be done individually or in small groups (up to four students). Each group chooses a real-world target. Targets must be legal to reverse-engineer in the context of security research. Many companies offer bug bounty programs that explicitly allow reverse engineering, such as Apple and Microsoft. In case vulnerabilities are found, students coordinately disclose them to the vendors, thereby improving the security of popular software. Instead of reverse engineering a particular target, students can also choose to develop reverse engineering tools.\\nStudents are free to choose their security research method. If applicable, they can use fuzzing and even collaborate with students from the course\\xa0Open-Source Fuzzing. While fuzzing is generally great for finding memory safety issues, there are further bug classes that require other approaches.', 'Dr. Jiska Classen', 'dh_master', 'wise-23-24-3892-reverse-engineering-for-security-analysis.html', 'HDAS', 'S', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3892-reverse-engineering-for-security-analysis.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3892-reverse-engineering-for-security-analysis.html', None, 'Reverse Engineering for Security Analysis', 6, 1.1, 'WiSe 2025/26', 'This seminar consists of two parts:\\nThe security research project can be done individually or in small groups (up to four students). Each group chooses a real-world target. Targets must be legal to reverse-engineer in the context of security research. Many companies offer bug bounty programs that explicitly allow reverse engineering, such as Apple and Microsoft. In case vulnerabilities are found, students coordinately disclose them to the vendors, thereby improving the security of popular software. Instead of reverse engineering a particular target, students can also choose to develop reverse engineering tools.\\nStudents are free to choose their security research method. If applicable, they can use fuzzing and even collaborate with students from the course\\xa0Open-Source Fuzzing. While fuzzing is generally great for finding memory safety issues, there are further bug classes that require other approaches.', 'Dr. Jiska Classen', 'dh_master', 'wise-23-24-3892-reverse-engineering-for-security-analysis.html', 'HDAS', 'C', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'Trends and Concepts in the Software Industry - Consulting the Business', 6, 1.1, 'WiSe 2025/26', None, 'Dr. Ralf Teusner, Dr. Ralf Teusner, Franziska Dobrigkeit, http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'dh_master', 'wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'SCAD', 'C', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'Trends and Concepts in the Software Industry - Consulting the Business', 6, 1.1, 'WiSe 2025/26', None, 'Dr. Ralf Teusner, Dr. Ralf Teusner, Franziska Dobrigkeit, http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'dh_master', 'wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'SCAD', 'T', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'Trends and Concepts in the Software Industry - Consulting the Business', 6, 1.1, 'WiSe 2025/26', None, 'Dr. Ralf Teusner, Dr. Ralf Teusner, Franziska Dobrigkeit, http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'dh_master', 'wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'SCAD', 'S', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'Trends and Concepts in the Software Industry - Consulting the Business', 6, 1.1, 'WiSe 2025/26', None, 'Dr. Ralf Teusner, Dr. Ralf Teusner, Franziska Dobrigkeit, http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'dh_master', 'wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'APAD', 'C', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'Trends and Concepts in the Software Industry - Consulting the Business', 6, 1.1, 'WiSe 2025/26', None, 'Dr. Ralf Teusner, Dr. Ralf Teusner, Franziska Dobrigkeit, http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'dh_master', 'wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'APAD', 'T', 'dh_master', 'Digital Health MA')\n",
      "('wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'https://hpi.de/studium/im-studium/lehrveranstaltungen/cybersecurity-ma/lehrveranstaltung/wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'Trends and Concepts in the Software Industry - Consulting the Business', 6, 1.1, 'WiSe 2025/26', None, 'Dr. Ralf Teusner, Dr. Ralf Teusner, Franziska Dobrigkeit, http://hpi.de/plattner/teaching/winter-term-2023-24/trends-and-concepts-in-the-software-industry-seminar.html', 'dh_master', 'wise-23-24-3898-trends-and-concepts-in-the-software-industry-_-consulting-the-business.html', 'APAD', 'S', 'dh_master', 'Digital Health MA')\n"
     ]
    }
   ],
   "source": [
    "for row in cursor.execute(\"\"\"\n",
    "            SELECT * \n",
    "            FROM modules \n",
    "            JOIN course_modules ON modules.url_trimmed = course_modules.module_url_trimmed \n",
    "            JOIN courses ON courses.course_abbreviation = course_modules.course_abbreviation\n",
    "            WHERE courses.course_abbreviation LIKE 'dh_master'\n",
    "            AND modules.evap_grade < 1.2;\n",
    "            \"\"\"):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cursor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      2\u001b[0m connection\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cursor' is not defined"
     ]
    }
   ],
   "source": [
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introduction_to_it_systems-yXmSL73L",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
